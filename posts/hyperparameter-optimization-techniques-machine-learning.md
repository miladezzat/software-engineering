---
card: "/images/default.jpg"
tags: [Machine Learning]
description: "When working on a machine learning project, you need to follo"
author: "Milad E. Fahmy"
title: "Hyperparameter Optimization Techniques to Improve Your Machine Learning Model s Performance"
created: "2021-08-16T15:35:24+02:00"
modified: "2021-08-16T15:35:24+02:00"
---
<div class="site-wrapper">
<main id="site-main" class="site-main outer">
<div class="inner">
<article class="post-full post tag-machine-learning tag-optimization tag-python ">
<header class="post-full-header">
<h1 class="post-full-title">Hyperparameter Optimization Techniques to Improve Your Machine Learning Model's Performance</h1>
</header>
<figure class="post-full-image">
<picture>
<source media="(max-width: 700px)" sizes="1px" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 1w">
<source media="(min-width: 701px)" sizes="(max-width: 800px) 400px,
(max-width: 1170px) 700px,
1400px" srcset="/news/content/images/size/w300/2020/10/freecodecampl-news-article-iamge.png 300w,
/news/content/images/size/w600/2020/10/freecodecampl-news-article-iamge.png 600w,
/news/content/images/size/w1000/2020/10/freecodecampl-news-article-iamge.png 1000w,
/news/content/images/size/w2000/2020/10/freecodecampl-news-article-iamge.png 2000w">
<img onerror="this.style.display='none'" src="/news/content/images/size/w2000/2020/10/freecodecampl-news-article-iamge.png" alt="Hyperparameter Optimization Techniques to Improve Your Machine Learning Model's Performance">
</picture>
</figure>
<section class="post-full-content">
<div class="post-content">
<p>When working on a machine learning project, you need to follow a series of steps until you reach your goal. </p><p>One of the steps you have to perform is hyperparameter optimization on your selected model. This task always comes after the model selection process where you choose the model that is performing better than other models.</p><h2 id="what-is-hyperparameter-optimization">What is hyperparameter optimization?</h2><p>Before I define hyperparameter optimization, you need to understand what a hyperparameter is. </p><p>In short, hyperparameters are different parameter values that are used to control the learning process and have a significant effect on the performance of machine learning models. </p><p>An example of hyperparameters in the Random Forest algorithm is the number of estimators (<em>n_estimators</em>), maximum depth (<em>max_depth</em>), and criterion. These parameters are <strong>tunable</strong> and can directly affect how well a model trains.</p><p>So then <strong>hyperparameter optimization </strong>is the process of finding the right combination of hyperparameter values to achieve maximum performance on the data in a reasonable amount of time. </p><p>This process plays a vital role in the prediction accuracy of a machine learning algorithm. Therefore Hyperparameter optimization is considered the <strong>trickiest</strong> part of building machine learning models.</p><p>Most of these machine learning algorithms come with the default values of their hyperparameters. But the default values do not always perform well on different types of Machine Learning projects. This is why you need to optimize them in order to get the right combination that will give you the best performance.</p><blockquote><em>A good choice of hyperparameters can really make an algorithm shine.</em></blockquote><p>There are some common strategies for optimizing hyperparameters. Let's look at each in detail now.</p><h2 id="how-to-optimize-hyperparameters">How to optimize hyperparameters</h2><h3 id="grid-search">Grid Search</h3><p>This is a widely used and traditional method that performs hyperparameter tuning to determine the optimal values for a given model. </p><p>Grid search works by trying every possible combination of parameters you want to try in your model. This means it will take<strong> a lot of time</strong> to perform the entire search which can get very computationally expensive.</p><p>You can learn more about how to implement Grid Search <a href="https://github.com/Davisy/Hyperparameter-Optimization-Techniques/blob/master/GridSearchCV%20.ipynb">here.</a></p><h3 id="random-search">Random Search</h3><p>This method works a bit differently: <strong>random</strong> combinations of the values of the hyperparameters are used to find the best solution for the built model. </p><p>The drawback of Random Search is that it can sometimes miss important points (values) in the search space.</p><p>You can learn more about how to implement Random Search <a href="https://github.com/Davisy/Hyperparameter-Optimization-Techniques/blob/master/RandomizedSearchCV.ipynb">here.</a></p><h2 id="alternative-hyperparameter-optimization-techniques">Alternative Hyperparameter Optimization techniques</h2><p>Now I will introduce you to a few alternative and advanced hyperparameter optimization techniques/methods. These can help you to obtain the best parameters for a given model.</p><p>We will look at the following techniques:</p><ol><li>Hyperopt</li><li>Scikit Optimize</li><li>Optuna</li></ol><h2 id="hyperopt">Hyperopt</h2><p>Hyperopt is a powerful Python library for hyperparameter optimization developed by James Bergstra. </p><p>It uses a form of Bayesian optimization for parameter tuning that allows you to get the best parameters for a given model. It can optimize a model with hundreds of parameters on a large scale.</p><p>Hyperopt has four important features you need to know in order to run your first optimization.</p><h3 id="search-space">Search Space</h3><p>Hyperopt has different functions to specify ranges for input parameters. These are called stochastic search spaces. The most common options for a search space are:</p><ul><li><strong>hp.choice(label, options)</strong> – This can be used for categorical parameters. It returns one of the options, which should be a list or tuple.<br>Example: <code>hp.choice("criterion", ["gini","entropy",])</code></li><li><strong>hp.randint(label, upper)</strong> – This can be used for Integer parameters. It returns a random integer in the range (0, upper).<br>Example: &nbsp;<code>hp.randint("max_features",50)</code></li><li><strong>hp.uniform(label, low, high)</strong> – This returns a value uniformly between <code>low</code> and <code>high</code>.<br>Example: <code>hp.uniform("max_leaf_nodes",1,10)</code></li></ul><p>Other option you can use are:</p><ul><li><strong>hp.normal(label, mu, sigma)</strong> –This returns a real value that's normally distributed with mean mu and standard deviation sigma</li><li><strong>hp.qnormal(label, mu, sigma, q)</strong> – This returns a value like round(normal(mu, sigma) / q) * q</li><li><strong>hp.lognormal(label, mu, sigma)</strong> – This returns a value drawn according to exp(normal(mu, sigma))</li><li><strong>hp.qlognormal(label, mu, sigma, q)</strong> – This returns a value like round(exp(normal(mu, sigma)) / q) * q</li></ul><p>You can learn more about search space options <a href="https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions">here.</a></p><p>Just a quick note: Every optimizable stochastic expression has a label (for example, n_estimators) as the first argument. These labels are used to return parameter choices to the caller during the optimization process.</p><h3 id="objective-function">Objective Function</h3><p>This is a minimization function that receives hyperparameter values as input from the search space and returns the loss.</p><p>This means that during the optimization process, we train the model with selected haypeparameter values and predict the target feature. Then we evaluate the prediction error and give it back to the optimizer. </p><p>The optimizer will decide which values to check and iterate again. You will learn how to create objective functions in the practical example.</p><h3 id="fmin">fmin</h3><p>The fmin function is the optimization function that iterates on different sets of algorithms and their hyperperameters and then minimizes the objective function.</p><p>fmin takes five inputs, which are:</p><ul><li>The objective function to minimize</li><li>The defined search space</li><li>The search algorithm to use, such as Random search, TPE (Tree Parzen Estimators) and Adaptive TPE<br>Note: <code>hyperopt.rand.suggest</code> and &nbsp;<code>hyperopt.tpe.suggest</code> provide logic for sequential search of the hyperparameter space</li><li>Maximum number of evaluations</li><li>And the trials object (optional)</li></ul><p>Example:</p><pre><code class="language-python">from hyperopt import fmin, tpe, hp,Trials
trials = Trials()
best = fmin(fn=lambda x: x ** 2,
space= hp.uniform('x', -10, 10),
algo=tpe.suggest,
max_evals=50,
trials = trials)
print(best)</code></pre><h3 id="trials-object">Trials Object </h3><p>The Trials object is used to keep all hyperparameters, loss, and other information. This means you can access it after running the optimization. </p><p>Also trials can help you save important information and later load and then resume the optimization process. You will learn more about this in the practical example below.</p><pre><code class="language-python">from hyperopt import Trials
trials = Trials()</code></pre><p>Now that you understand the important features of Hyperopt, we'll see how to use it. You'll follow these steps:</p><ul><li>Initialize the space over which to search</li><li>Define the objective function</li><li>Select the search algorithm to use</li><li>Run the hyperopt function</li><li>Analyze the evaluations outputs stored in the <strong>trials object</strong></li></ul><h3 id="hyperpot-in-practice">Hyperpot in Practice</h3><p>In this practical example, we will use the <strong>Mobile Price Dataset.</strong> Our task is to create a model that will predict &nbsp;how high the price of a mobile device will be: 0 (<em>low cost</em>), 1 (<em>medium cost</em>), 2 (<em>high cost</em>), or 3 (<em>very high cost</em>).</p><h3 id="install-hyperopt">Install Hyperopt</h3><p>You can install hyperopt from PyPI by running this command:</p><pre><code class="language-command">pip install hyperopt</code></pre><p>Then import the following important packages, including hyperopt:</p><pre><code class="language-python"># import packages
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from hyperopt import tpe, hp, fmin, STATUS_OK,Trials
from hyperopt.pyll.base import scope
import warnings
warnings.filterwarnings("ignore")</code></pre><h3 id="dataset">Dataset</h3><p>Let's load the dataset from the data directory. To get more information about the dataset, read about it <a href="https://www.kaggle.com/iabhishekofficial/mobile-price-classification?select=train.csv">here.</a></p><pre><code class="language-python"># load data
data = pd.read_csv("data/mobile_price_data.csv")</code></pre><p>Check the first five rows of the dataset like this:</p><pre><code class="language-python">#read data
data.shape</code></pre><p>We get the following:</p><p>(2000, 21)</p><p>In this dataset we have <em>2000 rows</em> and <em>21 columns</em>. Now let's understand the list of features we have in this dataset.</p><pre><code class="language-python">#show list of columns
list(data.columns)</code></pre><p>['battery_power', &nbsp;'blue', &nbsp;'clock_speed', &nbsp;'dual_sim', &nbsp;'fc', &nbsp;'four_g', &nbsp;'int_memory', &nbsp;'m_dep', &nbsp;'mobile_wt', &nbsp;'n_cores', &nbsp;'pc', &nbsp;'px_height', &nbsp;'px_width', &nbsp;'ram', &nbsp;'sc_h', &nbsp;'sc_w', &nbsp;'talk_time', &nbsp;'three_g', &nbsp;'touch_screen', &nbsp;'wifi', &nbsp;'price_range']</p><p>You can find the meaning of each column name <a href="https://www.kaggle.com/iabhishekofficial/mobile-price-classification">here</a> .</p><h3 id="splitting-the-dataset-into-target-feature-and-independent-features">Splitting the dataset into Target feature and Independent features</h3><p>This is a classification problem. So we will now split the target feature and independent features from the dataset. Our target feature is <strong>price_range</strong>.</p><pre><code class="language-python"># split data into features and target
X = data.drop("price_range", axis=1).values
y = data.price_range.values</code></pre><h3 id="preprocessing-the-dataset">Preprocessing the Dataset</h3><p>Next, we'll standardize the independent features by using the<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"> StandardScaler</a> method from scikit-learn.</p><pre><code class="language-python"># standardize the feature variables
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)</code></pre><h3 id="define-parameter-space-for-optimization">Define Parameter Space for Optimization</h3><p>We will use three hyperparameter of the <strong>Random Forest algorithm</strong>: <em>n_estimators, max_depth, </em>and<em> criterion</em>.</p><pre><code class="language-python">space = {
"n_estimators": hp.choice("n_estimators", [100, 200, 300, 400,500,600]),
"max_depth": hp.quniform("max_depth", 1, 15,1),
"criterion": hp.choice("criterion", ["gini", "entropy"]),
}</code></pre><p>We have set different values in the above selected hyperparameters. Now we will define the objective function.</p><h3 id="defining-a-function-to-minimize-objective-function-">Defining a Function to Minimize (Objective Function)</h3><p>Our function that we want to minimize is called <strong>hyperparamter_tuning</strong>. The classification algorithm to optimize its hyperparameter is <strong>Random Forest</strong>. </p><p>I use cross validation to avoid overfitting and then the function will return a loss values and its status. </p><pre><code class="language-python"># define objective function
def hyperparameter_tuning(params):
clf = RandomForestClassifier(**params,n_jobs=-1)
acc = cross_val_score(clf, X_scaled, y,scoring="accuracy").mean()
return {"loss": -acc, "status": STATUS_OK}</code></pre><p>Remember that <a href="https://github.com/hyperopt/hyperopt/tree/master/hyperopt" rel="noopener nofollow">hyperopt</a> minimizes the function. That's why I add the negative sign in the <strong>acc</strong>.</p><h3 id="fine-tune-the-model">Fine Tune the Model</h3><p>Finally, first we'll instantiate the Trial object, fine tune the model, and then print the best loss with its hyperparamters values.</p><pre><code class="language-python"># Initialize trials object
trials = Trials()
best = fmin(
fn=hyperparameter_tuning,
space = space,
algo=tpe.suggest,
max_evals=100,
trials=trials
)
print("Best: {}".format(best))</code></pre><p>100%|█████████████████████████████████████████████████████████| 100/100 [10:30&lt;00:00, &nbsp;6.30s/trial, best loss: -0.8915] Best: {'criterion': 1, 'max_depth': 11.0, 'n_estimators': 2}.</p><p>After performing hyperparamter optimization, the loss is <strong>- 0.8915</strong>. This means that the model performance has an accuracy of <strong>89.15%</strong> by using <em>n_estimators = 300, max_depth = 11, </em>and<em> criterion = "entropy"</em> in the Random Forest classifier.</p><h3 id="analyze-the-results-by-using-the-trials-object">Analyze the results by using the trials object</h3><p>The trials object can help us inspect all of the return values that were calculated during the experiment.</p><p><strong>(a) trials.results</strong><br>This show a list of dictionaries returned by 'objective' during the search.</p><pre><code class="language-python">trials.results</code></pre><p>[{'loss': -0.8790000000000001, 'status': 'ok'}, &nbsp;{'loss': -0.877, 'status': 'ok'}, &nbsp;{'loss': -0.768, 'status': 'ok'}, &nbsp;{'loss': -0.8205, 'status': 'ok'}, &nbsp;{'loss': -0.8720000000000001, 'status': 'ok'}, &nbsp;{'loss': -0.883, 'status': 'ok'}, &nbsp;{'loss': -0.8554999999999999, 'status': 'ok'}, &nbsp;{'loss': -0.8789999999999999, 'status': 'ok'}, &nbsp;{'loss': -0.595, 'status': 'ok'},.......]</p><p><strong>(b) trials.losses()</strong><br>This shows a list of losses (float for each 'ok' trial).</p><pre><code class="language-python">trials.losses()</code></pre><p><br>[-0.8790000000000001, &nbsp;-0.877, &nbsp;-0.768, &nbsp;-0.8205, &nbsp;-0.8720000000000001, &nbsp;-0.883, &nbsp;-0.8554999999999999, &nbsp;-0.8789999999999999, &nbsp;-0.595, &nbsp;-0.8765000000000001, &nbsp;-0.877, .........]</p><p><strong>(c) trials.statuses()</strong><br>This shows a list of status strings.</p><pre><code class="language-python">trials.statuses()</code></pre><p>['ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', &nbsp;'ok', ..........]</p><p>Note: This trials object can be saved, passed on to the built-in plotting routines, or analyzed with your own custom code.</p><p>Now that you know how to implement Hyperopt, let's learn the second alternative hyperparameter optimization technique called <strong><strong>Scikit-Optimize</strong></strong>.</p><h2 id="scikit-optimize">Scikit-Optimize</h2><p>Scikit-optimize is another open-source Python library for hyperparameter optimization. It implements several methods for sequential model-based optimization. </p><p>The library is very easy to use and provides a general toolkit for Bayesian optimization that can be used for hyperparameter tuning. It also provides support for tuning the hyperparameters of machine learning algorithms offered by the scikit-learn library.</p><p>The scikit-optimize is built on top of Scipy, NumPy, and Scikit-Learn.</p><p>Scikit-optimize has at least four important features you need to know in order to run your first optimization. Let's look at them in depth now.</p><h3 id="space">Space</h3><p>scikit-optimize has different functions to define the optimization space which contains one or multiple dimensions. The most common options for a search space to choose are:</p><ul><li><strong><strong>Real </strong></strong>— This is a search space dimension that can take on any real value. You need to define the lower bound and upper bound and both are inclusive.<br>Example: <code>Real(low=0.2, high=0.9, name="min_samples_leaf")</code></li><li><strong><strong>Integer </strong></strong>— This is a search space dimension that can take on integer values.<br>Example: <code>Integer(low=3, high=25, name="max_features")</code></li><li><strong><strong>Categorical</strong></strong> — This is a search space dimension that can take on categorical values.<br>Example: <code>Categorical(["gini","entropy"],name="criterion")</code></li></ul><p>Note: in each search space you have to define the hyperparameter name to optimize by using the <strong><strong>name </strong></strong>argument.</p><h3 id="bayessearchcv">BayesSearchCV</h3><p>The BayesSearchCV class provides an interface similar to <code>GridSearchCV</code> or <code>RandomizedSearchCV</code> but it performs Bayesian optimization over hyperparameters. </p><p>BayesSearchCV implements a “<strong><strong>fit</strong></strong>” and a “<strong><strong>score</strong></strong>” method and other common methods like <em><em>predict(),predict_proba(), decision_function(), transform() </em></em>and<em><em> inverse_transform()</em></em> if they are implemented in the estimator used.</p><p>In contrast to GridSearchCV, not all parameter values are tried out. Rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.</p><p>Note that<strong><strong> </strong></strong>you will learn how to implement BayesSearchCV in a practical example below.</p><h3 id="objective-function-1">Objective Function</h3><p>This is a function that will be called by the search procedure. It receives hyperparameter values as input from the search space and returns the loss (the lower the better). </p><p>This means that during the optimization process, we train the model with selected hyperparameter values and predict the target feature. Then we evaluate the prediction error and give it back to the optimizer. </p><p>The optimizer will decide which values to check and iterate over again. You will learn how to create an objective function in the practical example below.</p><h3 id="optimizer">Optimizer</h3><p>This is the function that performs the Bayesian Hyperparameter Optimization process. The optimization function iterates at each model and the search space to optimize and then minimizes the objective function.</p><p>There are different optimization functions provided by the scikit-optimize library, such as:</p><ul><li><strong><strong>dummy_minimize</strong></strong> — Random search by uniform sampling within the given bounds.</li><li><strong><strong>forest_minimize</strong></strong> — Sequential optimization using decision trees.</li><li><strong><strong>gbrt_minimize</strong></strong> — Sequential optimization using gradient boosted trees.</li><li><strong><strong>gp_minimize — </strong></strong>Bayesian optimization using Gaussian Processes.<br>Note: we will implement gp_minimize in the practical example below.</li></ul><p>Other features you should learn are as follow:</p><ul><li><a href="https://scikit-optimize.github.io/0.7/modules/classes.html#module-skopt.space.transformers" rel="noopener nofollow">Space Transformer</a>s</li><li><a href="https://scikit-optimize.github.io/0.7/modules/classes.html#module-skopt.utils" rel="noopener nofollow">Utils Functions</a></li><li><a href="https://scikit-optimize.github.io/0.7/modules/classes.html#module-skopt.plots" rel="noopener nofollow">Plotting Functions</a></li><li><a href="https://scikit-optimize.github.io/0.7/modules/classes.html#module-skopt.learning" rel="noopener nofollow">Machine learning extensions for model-based optimization</a></li></ul><h3 id="scikit-optimize-in-practice">Scikit-optimize in Practice</h3><p>Now that you know the important features of scikit-optimize, let's look at a practical example. We will use the same dataset called <strong><strong>Mobile Price Dataset</strong></strong> that we used with Hyperopt.</p><h3 id="install-scikit-optimize">Install Scikit-Optimize</h3><p>scikit-optimize requires the following Python version and packages:</p><ul><li>Python &gt;= 3.6</li><li>NumPy (&gt;= 1.13.3)</li><li>SciPy (&gt;= 0.19.1)</li><li>joblib (&gt;= 0.11)</li><li>scikit-learn &gt;= 0.20</li><li>matplotlib &gt;= 2.0.0</li></ul><p>You can install the latest release with this command:</p><pre><code class="language-command">pip install scikit-optimize</code></pre><p>Then import important packages, including scikit-optimize:</p><pre><code class="language-python">
# import packages
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from skopt.searchcv import BayesSearchCV
from skopt.space import Integer, Real, Categorical
from skopt.utils import use_named_args
from skopt import gp_minimize
import warnings
warnings.filterwarnings("ignore")</code></pre><h3 id="the-first-approach">The First Approach</h3><p>In the first approach, we will use <strong><strong>BayesSearchCV</strong></strong> to perform hyperparameter optimization for the Random Forest algorithm.</p><h3 id="define-search-space">Define Search Space</h3><p>We will tune the following hyperparameters of the Random Forest model:</p><ul><li><strong><strong>n_estimators </strong></strong>— The number of trees in the forest.</li><li><strong><strong>max_depth</strong></strong> — The maximum depth of the tree.</li><li><strong><strong>criterion</strong></strong> — The function to measure the quality of a split.</li></ul><pre><code class="language-python"># define search space
params = {
"n_estimators": [100, 200, 300, 400],
"max_depth": (1, 9),
"criterion": ["gini", "entropy"],
}</code></pre><p>We have defined the search space as a dictionary. It has hyperparameter names used as the key, and the scope of the variable as the value.</p><h3 id="define-the-bayessearchcv-configuration">Define the BayesSearchCV configuration</h3><p>The benefit of BayesSearchCV is that the search procedure is performed automatically, which requires minimal configuration. </p><p>The class can be used in the same way as Scikit-Learn (GridSearchCV and RandomizedSearchCV).</p><pre><code class="language-python"># define the search
search = BayesSearchCV(
estimator=rf_classifier,
search_spaces=params,
n_jobs=1,
cv=5,
n_iter=30,
scoring="accuracy",
verbose=4,
random_state=42
)</code></pre><h3 id="fine-tune-the-model-1">Fine Tune the Model</h3><p>We then execute the search by passing the preprocessed features and the target feature (price_range).</p><pre><code class="language-python"># perform the search
search.fit(X_scaled,y)</code></pre><p>You can find the best score by using the <strong><strong>best_score_</strong></strong> attribute and the best parameters by using <strong><strong>best_params_</strong></strong> attribute from the <strong><strong>search</strong></strong>.</p><pre><code class="language-python"># report the best result
print(search.best_score_)
print(search.best_params_)</code></pre><p>Note that the current version of scikit-optimize (0.7.4) is not compatible with the latest versions of scikit learn (0.23.1 and 0.23.2). So when you run the optimization process using this approach, you can get errors like this:</p><pre><code class="language-commanda">TypeError: object.__init__() takes exactly one argument (the instance to initialize)</code></pre><p>You can find more information about this error in their GitHub account.</p><ul><li><a href="https://github.com/scikit-optimize/scikit-optimize/issues/928" rel="noopener nofollow">https://github.com/scikit-optimize/scikit-optimize/issues/928</a></li><li><a href="https://github.com/scikit-optimize/scikit-optimize/issues/924" rel="noopener nofollow">https://github.com/scikit-optimize/scikit-optimize/issues/924</a></li><li><a href="https://github.com/scikit-optimize/scikit-optimize/issues/902" rel="noopener nofollow">https://github.com/scikit-optimize/scikit-optimize/issues/902</a></li></ul><p>I hope they will solve this incompatibility problem very soon.</p><h3 id="the-second-approach">The Second Approach</h3><p>In the second approach, we first define the search space by using the space methods provided by scikit-optimize, which are <em><em>Categorical and Integer.</em></em></p><pre><code class="language-python"># define the space of hyperparameters to search
search_space = list()
search_space.append(Categorical([100, 200, 300, 400], name='n_estimators'))
search_space.append(Categorical(['gini', 'entropy'], name='criterion'))
search_space.append(Integer(1, 9, name='max_depth'))</code></pre><p>We have set different values in the above-selected hyperparameters. Then we will define the objective function.</p><h3 id="defining-a-function-to-minimize-objective-function--1">Defining a Function to Minimize (Objective Function)</h3><p>Our function to minimize is called <strong><strong>evalute_model</strong></strong> and the classification algorithm to optimize its hyperparameter is <strong><strong>Random Forest</strong></strong>. </p><p>I use cross-validation to avoid overfitting and then the function will return loss values.</p><pre><code class="language-python"># define the function used to evaluate a given configuration
@use_named_args(search_space)
def evaluate_model(**params):
# configure the model with specific hyperparameters
clf = RandomForestClassifier(**params, n_jobs=-1)
acc = cross_val_score(clf, X_scaled, y, scoring="accuracy").mean()</code></pre><p>The<strong><strong> use_named_args() </strong></strong>decorator allows your objective function to receive the parameters as keyword arguments. This is particularly convenient when you want to set scikit-learn's estimator parameters.</p><p>Remember that scikit-optimize minimizes the function, which is why I add a negative sign in the <strong><strong>acc.</strong></strong></p><h3 id="fine-tune-the-model-2">Fine Tune the Model</h3><p>Finally, we fine-tune the model by using the <strong><strong>gp_minimize</strong></strong> method (it uses Gaussian process-based optimization) from scikit-optimize. Then we print the best loss with its hyperparameters values.</p><pre><code class="language-python"># perform optimization
result = gp_minimize(
func=evaluate_model,
dimensions=search_space,
n_calls=30,
random_state=42,
verbose=True,
n_jobs=1,
)</code></pre><p><strong><strong>Output:</strong></strong><br><em><em>Iteration No: 1 started. Evaluating function at random point.</em></em><br><em><em>Iteration No: 1 ended. Evaluation done at random point.</em></em><br><em><em>Time taken: 8.6910</em></em><br><em><em>Function value obtained: -0.8585</em></em><br><em><em>Current minimum: -0.8585</em></em><br><em><em>Iteration No: 2 started. Evaluating function at random point.</em></em><br><em><em>Iteration No: 2 ended. Evaluation done at random point.</em></em><br><em><em>Time taken: 4.5096</em></em><br><em><em>Function value obtained: -0.7680</em></em><br><em><em>Current minimum: -0.8585 ………………….</em></em></p><p>Not that it will run until it reaches the last iteration. For our optimization process, the total number of iterations is 30.</p><p>Then we can print the best accuracy and the values of the selected hyperparameters we used.</p><pre><code class="language-python"># summarizing finding:
print('Best Accuracy: %.3f' % (result.fun))
print('Best Parameters: %s' % (result.x))</code></pre><pre><code class="language-command">Best Accuracy: -0.882
Best Parameters: [300, 'entropy', 9]</code></pre><p>After performing hyperparameter optimization, the loss is <strong><strong>-0.882</strong></strong>. This means that the model's performance has an accuracy of <strong><strong>88.2%</strong></strong> by using <em><em>n_estimators = 300,</em> <em>max_depth = 9, </em></em>and<em><em> criterion = “entropy”</em></em> in the Random Forest classifier.</p><p>Our result is not much different from Hyperopt in the first part (accuracy of <strong><strong>89.15%</strong></strong>).</p><h3 id="print-function-values">Print Function Values</h3><p>You can print all function values at each iteration by using the<strong><strong> func_vals</strong></strong> attribute from the OptimizeResult object (result).</p><pre><code class="language-python">print(result.func_vals)</code></pre><p><strong><strong>Output:</strong></strong><br>array([-0.8665, -0.7765, -0.7485, -0.86 , -0.872 , -0.545 , -0.81 ,<br>-0.7725, -0.8115, -0.8705, -0.8685, -0.879 , -0.816 , -0.8815,<br>-0.8645, -0.8745, -0.867 , -0.8785, -0.878 , -0.878 , -0.8785,<br>-0.874 , -0.875 , -0.8785, -0.868 , -0.8815, -0.877 , -0.879 ,<br>-0.8705, -0.8745])</p><h3 id="plot-convergence-traces">Plot Convergence Traces</h3><p>We can use the <strong><strong>plot_convergence</strong></strong> method from scikit-optimize to plot one or several convergence traces. We just need to pass the OptimizeResult object (result) in the plot_convergence method.</p><pre><code class="language-python"># plot convergence
from skopt.plots import plot_convergence
# Define the search space
criterions = trial.suggest_categorical('criterion', ['gini', 'entropy'])
max_depths = trial.suggest_int('max_depth', 1, 9, 1)
n_estimators = trial.suggest_int('n_estimators', 100, 1000, 100)
clf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_estimators,
criterion=criterions,
max_depth=max_depths,
n_jobs=-1)
score = cross_val_score(clf, X_scaled, y, scoring="accuracy").mean()
return score</code></pre><p>(d) Study</p><p>A study corresponds to an optimization task (a set of trials). If you need to start the optimization process, you need to create a study object and pass the objective function to a method called <strong><strong>optimize()</strong></strong> and set the number of trials as follows:</p><pre><code class="language-python">study = optuna.create_study()
study.optimize(objective, n_trials=100)</code></pre><p> <br>The <strong><strong>create_study()</strong></strong> method allows you to choose whether you want to <em><em>maximize </em></em>or <em><em>minimize </em></em>your objective function. </p><p>This is one of the more useful features I like in optuna because you have the ability to choose the direction of the optimization process.</p><p>Note that you will learn how to implement this in the practical example below.</p><h3 id="visualization">Visualization</h3><p>The visualization module in Optuna provides different methods to create figures for the optimization outcome. These methods help you gain information about interactions between parameters and let you know how to move forward. </p><p>Here are some of the methods you can use.</p><ul><li><strong><strong>plot_contour()</strong> </strong>– This method plots the parameter relationship as a contour plot in a study.</li><li><strong><strong>plot_intermidiate_values() </strong></strong>–<strong><strong> </strong></strong>This method plots intermediate values of all trials in a study.</li><li><strong><strong>plot_optimization_history() </strong></strong>–<strong><strong> </strong></strong>This method plots the optimization history of all trials in a study.</li><li><strong><strong>plot_param_importances() </strong></strong>–<strong><strong> </strong></strong>This method plots hyperparameter importance and their values.</li><li><strong><strong>plot_edf()</strong> </strong>–<strong><strong> </strong></strong>This method plots the objective value EDF (empirical distribution function) of a study.</li></ul><p>We will use some of the methods mentioned above in the practical example below.</p><h3 id="optuna-in-practice">Optuna in Practice</h3><p>Now that you know the important features of &nbsp;Optuna, in this practical example we will use the same dataset (<strong><strong>Mobile Price Dataset</strong></strong>) that we used in the previous two methods above.</p><h3 id="install-optuna">Install Optuna</h3><p>You can install the latest release with:</p><pre><code class="language-command">pip install optuna</code></pre><p>Then import the important packages, including optuna:</p><pre><code class="language-python"># import packages
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
import joblib
import optuna
from optuna.samplers import TPESampler
import warnings
warnings.filterwarnings("ignore")</code></pre><h3 id="define-search-space-and-objective-in-one-function">Define search space and objective in one function</h3><p>As I have explained above, Optuna allows you to define the search space and objective in one function. </p><p>We will define the search spaces for the following hyperparameters of the Random Forest model:</p><ul><li><strong><strong>n_estimators </strong></strong>— The number of trees in the forest.</li><li><strong><strong>max_depth</strong></strong> — The maximum depth of the tree.</li><li><strong><strong>criterion</strong></strong> — The function to measure the quality of a split.</li></ul><pre><code class="language-python"># define the search space and the objecive function
def objective(trial):
# Define the search space
criterions = trial.suggest_categorical('criterion', ['gini', 'entropy'])
max_depths = trial.suggest_int('max_depth', 1, 9, 1)
n_estimators = trial.suggest_int('n_estimators', 100, 1000, 100)
clf = RandomForestClassifier(n_estimators=n_estimators,
criterion=criterions,
max_depth=max_depths,
n_jobs=-1)
score = cross_val_score(clf, X_scaled, y, scoring="accuracy").mean()
return score</code></pre><p>We will use the <strong><strong>trial.suggest_categorical()</strong></strong> method to define a search space for <em><em>criterion</em></em> and <strong><strong>trial.suggest_int()</strong></strong> for both <em><em>max_depth</em></em> and <em><em>n_estimators</em></em>.</p><p>Also, we will use cross-validation to avoid overfitting, and then the function will return the mean accuracy.</p><h3 id="create-a-study-object">Create a Study Object</h3><p>Next we create a study object that corresponds to the optimization task. The <strong><strong>create-study() </strong></strong>method allows us to provide the name of the study, the direction of the optimization (<em><em>maximize </em></em>or<em><em> minimize</em></em>), and the optimization method we want to use.</p><pre><code class="language-python"># create a study object
study = optuna.create_study(study_name="randomForest_optimization",
direction="maximize",
sampler=TPESampler())</code></pre><p>In our case we named our study object <strong><strong>randomForest_optimization</strong></strong>. The direction of the optimization is <strong><strong>maximize</strong></strong> (which means the higher the score the better) and the optimization method to use is <strong><strong>TPESampler().</strong></strong></p><h3 id="fine-tune-the-model-3">Fine Tune the Model</h3><p>To run the optimization process, we need to pass the objective function and number of trials in the <strong><strong>optimize()</strong></strong> method from the study object we have created. </p><p>We have set the number of trials to be 10 (but you can change the number if you want to run more trials).</p><pre><code class="language-python">
# pass the objective function to method optimize()
joblib.dump(study, 'optuna_searches/study.pkl')</code></pre><p>Then if you want to load the hyperparameter searches from the optuna_searches directory, you can use the <strong><strong>load()</strong></strong> method from joblib.</p><pre><code class="language-python">
# load your hyperparameter searches
</div>
<hr>
<hr>
</section>
</article>
</div>
</main>
</div>
<!-- Google Tag Manager (noscript) -->
<!-- End Google Tag Manager (noscript) -->
